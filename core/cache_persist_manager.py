# -*- coding: utf-8 -*-
"""
Created on Thu Oct 10 10:33:12 2024

@author: Xintang Zheng

æ˜Ÿæ˜Ÿ: â˜… â˜† âœª âœ© ğŸŒŸ â­ âœ¨ ğŸŒ  ğŸ’« â­ï¸
å‹¾å‹¾å‰å‰: âœ“ âœ” âœ• âœ– âœ… â
æŠ¥è­¦å•¦: âš  â“˜ â„¹ â˜£
ç®­å¤´: â” âœ â™ â¤ â¥ â†© â†ª
emoji: ğŸ”” â³ â° ğŸ”’ ğŸ”“ ğŸ›‘ ğŸš« â— â“ âŒ â­• ğŸš€ ğŸ”¥ ğŸ’§ ğŸ’¡ ğŸµ ğŸ¶ ğŸ§­ ğŸ“… ğŸ¤” ğŸ§® ğŸ”¢ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ§  ğŸ“

"""
# %% imports
import pandas as pd
from abc import ABC, abstractmethod
import threading
from collections import defaultdict
from pathlib import Path
from datetime import datetime, timedelta


from utility.timeutils import get_curr_utc_date, get_date_based_on_timestamp
from utility.datautils import add_row_to_dataframe_reindex


# %%
class ParquetManager(ABC):
    
    def __init__(self, log=None):
        self.log = log
        self.locks = defaultdict(threading.Lock)  # åˆå§‹åŒ–é”
        
    def _add_row(self, dataframe, new_row, index):
        dataframe = add_row_to_dataframe_reindex(dataframe, new_row, index)
        return dataframe

    def _save_to_parquet(self, path, dataframe):
        dataframe.to_parquet(path)
        if self.log:
            self.log.success(f'Successfully saved data to {path}')

    def _load_from_parquet(self, path):
        if path.exists():
            try:
                dataframe = pd.read_parquet(path)
                if self.log:
                    self.log.success(f'Successfully loaded Parquet from {path}')
                return dataframe
            except Exception as e:
                if self.log:
                    self.log.warning(f'Parquet file unreadable: {path}. Error: {e}')
        return pd.DataFrame()  # Return an empty DataFrame if loading fails
    
    
class CacheManager(ParquetManager):
    
    def __init__(self, cache_dir, cache_lookback, cache_list=[], log=None):
        super().__init__(log=log)
        self.cache_dir = cache_dir  # ç¡®ä¿ cache_dir æ˜¯ Path å¯¹è±¡
        self.cache_lookback = cache_lookback
        self.cache_list = cache_list
        
        if self.cache_list and self.log:
            self.log.success(f'Cache List Registered: {self.cache_list}')

        self._init_containers()
        self._load_or_init_cache()
        
    def _init_containers(self):
        self.cache = defaultdict(pd.DataFrame)

    def _load_or_init_cache(self):
        for cache_name in self.cache_list:
            cache_path = self.cache_dir / f'{cache_name}.parquet'
            self.cache[cache_name] = self._load_from_parquet(cache_path)

    def _cut_cache(self, cache_name, curr_ts):
        cut_time = pd.Timestamp(curr_ts - self.cache_lookback)
        self.cache[cache_name] = self.cache[cache_name].loc[cut_time:]
            
    def save(self, ts):
        for cache_name in self.cache_list:
            with self.locks[cache_name]:
                self._cut_cache(cache_name, ts)  # ä¿ç•™ cut_cache æ­¥éª¤
            
            cache_path = self.cache_dir / f'{cache_name}.parquet'
            self._save_to_parquet(cache_path, self.cache[cache_name])
            
    def add_row(self, cache_name, new_row, index):
        with self.locks[cache_name]:
            self.cache[cache_name] = self._add_row(self.cache[cache_name], new_row, index)
        
    def __getitem__(self, cache_name):
        return self.cache[cache_name]
    
    def __setitem__(self, cache_name, data):
        self.cache[cache_name] = data


class PersistManager(ParquetManager):
    
    def __init__(self, persist_dir, persist_list=[], log=None):
        super().__init__(log=log)
        self.persist_dir = persist_dir  # ç¡®ä¿ persist_dir æ˜¯ Path å¯¹è±¡
        
        self._init_containers()
        self._init_persist_list(persist_list)
        self._load_or_init_persist()
        
    def _init_containers(self):
        self.factor_persist = defaultdict(pd.DataFrame)
        # æ–°å¢ï¼šç¼“å­˜å·²åŠ è½½çš„å†å²æ•°æ®ï¼Œé¿å…é‡å¤è¯»å–æ–‡ä»¶
        self._historical_data_cache = defaultdict(dict)  # {data_name: {date: DataFrame}}
        
    def _init_persist_list(self, persist_list):
        self.persist_list = persist_list
        
        if self.persist_list and self.log:
            self.log.success(f'Persist List Registered: {self.persist_list}')
            
        for persist_name in persist_list:
            persist_name_dir = self.persist_dir / persist_name
            persist_name_dir.mkdir(parents=True, exist_ok=True)
        
    def _load_or_init_persist(self):
        date_to_load = get_curr_utc_date()  # æ ¹æ®å½“å‰æ—¥æœŸç”Ÿæˆæ–‡ä»¶å
        for key in self.persist_list:
            path = self.persist_dir / key / f'{date_to_load}.parquet'
            self.factor_persist[key] = self._load_from_parquet(path)
    
    def save(self, ts):
        date_to_save = get_date_based_on_timestamp(ts)  # æ ¹æ®ä¼ å…¥æ—¶é—´æˆ³ç”Ÿæˆæ—¥æœŸ
        for key in self.persist_list:
            with self.locks[key]:
                self.factor_persist[key] = self.factor_persist[key].loc[date_to_save:]  # ç›´æ¥è£å‰ªå½“å¤©çš„æ•°æ®
                
                # ä¿å­˜åˆ° Parquet æ–‡ä»¶
                path = self.persist_dir / key / f'{date_to_save}.parquet'
                self._save_to_parquet(path, self.factor_persist[key])
        
    def add_row(self, key, new_row, index):
        with self.locks[key]:
            self.factor_persist[key] = self._add_row(self.factor_persist[key], new_row, index)
    
    def has_data(self, data_name, timestamp):
        """æ£€æŸ¥ç‰¹å®šæ—¶é—´æˆ³çš„æ•°æ®æ˜¯å¦å­˜åœ¨"""
        try:
            # é¦–å…ˆæ£€æŸ¥å½“å‰å†…å­˜ä¸­çš„æ•°æ®
            current_data = self.factor_persist.get(data_name)
            if current_data is not None and timestamp in current_data.index:
                return True
            
            # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ï¼Œæ£€æŸ¥å†å²æ–‡ä»¶
            target_date = get_date_based_on_timestamp(timestamp)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜è¯¥æ—¥æœŸçš„æ•°æ®
            if target_date in self._historical_data_cache[data_name]:
                cached_data = self._historical_data_cache[data_name][target_date]
                return timestamp in cached_data.index
            
            # å°è¯•åŠ è½½è¯¥æ—¥æœŸçš„æ–‡ä»¶
            historical_data = self._load_historical_data(data_name, target_date)
            if historical_data is not None:
                return timestamp in historical_data.index
            
            return False
            
        except Exception as e:
            if self.log:
                self.log.warning(f"Error checking data existence for {data_name} at {timestamp}: {e}")
            return False

    def get_row(self, data_name, timestamp):
        """è·å–ç‰¹å®šæ—¶é—´æˆ³çš„æ•°æ®è¡Œ"""
        try:
            # é¦–å…ˆæ£€æŸ¥å½“å‰å†…å­˜ä¸­çš„æ•°æ®
            current_data = self.factor_persist.get(data_name)
            if current_data is not None and timestamp in current_data.index:
                return current_data.loc[timestamp]
            
            # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ï¼Œä»å†å²æ–‡ä»¶ä¸­è·å–
            target_date = get_date_based_on_timestamp(timestamp)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜è¯¥æ—¥æœŸçš„æ•°æ®
            if target_date in self._historical_data_cache[data_name]:
                cached_data = self._historical_data_cache[data_name][target_date]
                if timestamp in cached_data.index:
                    return cached_data.loc[timestamp]
            
            # å°è¯•åŠ è½½è¯¥æ—¥æœŸçš„æ–‡ä»¶
            historical_data = self._load_historical_data(data_name, target_date)
            if historical_data is not None and timestamp in historical_data.index:
                return historical_data.loc[timestamp]
            
            return None
            
        except Exception as e:
            if self.log:
                self.log.warning(f"Error getting data for {data_name} at {timestamp}: {e}")
            return None

    def _load_historical_data(self, data_name, target_date):
        """åŠ è½½æŒ‡å®šæ—¥æœŸçš„å†å²æ•°æ®å¹¶ç¼“å­˜"""
        try:
            # å¦‚æœå·²ç»ç¼“å­˜ï¼Œç›´æ¥è¿”å›
            if target_date in self._historical_data_cache[data_name]:
                return self._historical_data_cache[data_name][target_date]
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            historical_path = self.persist_dir / data_name / f'{target_date}.parquet'
            
            # åŠ è½½æ•°æ®
            historical_data = self._load_from_parquet(historical_path)
            
            # ç¼“å­˜æ•°æ®ï¼ˆå³ä½¿æ˜¯ç©ºDataFrameä¹Ÿç¼“å­˜ï¼Œé¿å…é‡å¤å°è¯•è¯»å–ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼‰
            self._historical_data_cache[data_name][target_date] = historical_data
            
            if self.log and not historical_data.empty:
                self.log.info(f"Loaded historical data for {data_name} on {target_date}: {len(historical_data)} rows")
            
            return historical_data if not historical_data.empty else None
            
        except Exception as e:
            if self.log:
                self.log.warning(f"Error loading historical data for {data_name} on {target_date}: {e}")
            # ç¼“å­˜Noneä»¥é¿å…é‡å¤å°è¯•
            self._historical_data_cache[data_name][target_date] = pd.DataFrame()
            return None

    def get_data_range(self, data_name, start_timestamp, end_timestamp):
        """è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®"""
        try:
            all_data = []
            
            # è®¡ç®—éœ€è¦çš„æ—¥æœŸèŒƒå›´
            start_date = get_date_based_on_timestamp(start_timestamp)
            end_date = get_date_based_on_timestamp(end_timestamp)
            
            current_date = datetime.strptime(start_date, '%Y%m%d')
            end_date_dt = datetime.strptime(end_date, '%Y%m%d')
            
            # éå†æ¯ä¸€å¤©
            while current_date <= end_date_dt:
                date_str = current_date.strftime('%Y%m%d')
                
                # å¦‚æœæ˜¯å½“å‰æ—¥æœŸï¼Œä½¿ç”¨å†…å­˜æ•°æ®
                if date_str == get_curr_utc_date():
                    current_data = self.factor_persist.get(data_name)
                    if current_data is not None and not current_data.empty:
                        all_data.append(current_data)
                else:
                    # å¦åˆ™åŠ è½½å†å²æ•°æ®
                    historical_data = self._load_historical_data(data_name, date_str)
                    if historical_data is not None:
                        all_data.append(historical_data)
                
                current_date += timedelta(days=1)
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            if all_data:
                combined_data = pd.concat(all_data).sort_index()
                # è¿‡æ»¤åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´
                return combined_data.loc[start_timestamp:end_timestamp]
            
            return pd.DataFrame()
            
        except Exception as e:
            if self.log:
                self.log.warning(f"Error getting data range for {data_name}: {e}")
            return pd.DataFrame()

    def clear_historical_cache(self):
        """æ¸…ç†å†å²æ•°æ®ç¼“å­˜ï¼ˆåœ¨éœ€è¦æ—¶é‡Šæ”¾å†…å­˜ï¼‰"""
        self._historical_data_cache.clear()
        if self.log:
            self.log.info("Historical data cache cleared")

    def get_cache_info(self):
        """è·å–ç¼“å­˜ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        cache_info = {}
        for data_name, date_cache in self._historical_data_cache.items():
            cache_info[data_name] = {
                'cached_dates': list(date_cache.keys()),
                'total_rows': sum(len(df) for df in date_cache.values() if not df.empty)
            }
        return cache_info